# üöÄ Scraping Performance Optimization Guide

## Overview
This document outlines the performance optimizations implemented to significantly improve scraping job speed.

## üö® **Major Performance Issues Identified & Fixed**

### 1. **Sequential Processing (FIXED)**
- **Before**: Products processed one by one with delays between each
- **After**: Concurrent processing with configurable batch sizes
- **Improvement**: 5-8x faster processing for multiple products

### 2. **Excessive Rate Limiting (FIXED)**
- **Before**: 1000ms (1 second) delay between requests
- **After**: 100-200ms delay between batches
- **Improvement**: 5-10x reduction in wait time

### 3. **Puppeteer Overuse (OPTIMIZED)**
- **Before**: Full Puppeteer for every page with long timeouts
- **After**: Smart Puppeteer usage with resource blocking and reduced timeouts
- **Improvement**: 2-3x faster page loading

### 4. **Multiple Wait Operations (OPTIMIZED)**
- **Before**: Waiting for all selectors with 5-second timeouts
- **After**: Smart waiting for essential selectors only with 2-3 second timeouts
- **Improvement**: 2-4x faster element detection

## ‚ö° **Performance Optimizations Implemented**

### **Concurrent Processing**
```typescript
// Before: Sequential processing
for (let i = 0; i < productUrls.length; i++) {
  await adapter.extractProduct(url);
  await this.delay(1000); // 1 second delay
}

// After: Concurrent processing
const maxConcurrent = recipe.behavior?.maxConcurrent || 5;
for (let i = 0; i < productUrls.length; i += maxConcurrent) {
  const batch = productUrls.slice(i, i + maxConcurrent);
  const batchPromises = batch.map(async (url) => {
    return await adapter.extractProduct(url);
  });
  await Promise.all(batchPromises);
  await this.delay(200); // Only between batches
}
```

### **Smart Rate Limiting**
```yaml
# Before
behavior:
  rateLimit: 1000  # 1 second between each request
  maxConcurrent: 1  # Process one at a time

# After
behavior:
  rateLimit: 200   # 200ms between batches
  maxConcurrent: 5  # Process 5 concurrently
```

### **Puppeteer Optimizations**
```typescript
// Resource blocking for faster loading
await page.setRequestInterception(true);
page.on('request', (req) => {
  if (['image', 'stylesheet', 'font', 'media'].includes(req.resourceType())) {
    req.abort(); // Block unnecessary resources
  } else {
    req.continue();
  }
});

// Reduced timeouts
await page.goto(url, { 
  waitUntil: 'domcontentloaded',
  timeout: 10000  // Reduced from 15000
});

// Smart selector waiting
const essentialSelectors = options.waitForSelectors.slice(0, 3);
await page.waitForSelector(selector, { timeout: 3000 }); // Reduced from 5000
```

## üìä **Expected Performance Improvements**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **100 Products** | ~100 seconds | ~20 seconds | **5x faster** |
| **Rate Limiting** | 1000ms | 100-200ms | **5-10x faster** |
| **Concurrent Processing** | 1 product | 5-8 products | **5-8x faster** |
| **Page Load Time** | 15-20 seconds | 5-10 seconds | **2-3x faster** |
| **Total Scraping Time** | ~2-3 minutes | ~20-30 seconds | **4-6x faster** |

## üîß **Configuration Recommendations**

### **For High-Performance Sites (Good servers, fast response)**
```yaml
behavior:
  rateLimit: 50      # Very aggressive
  maxConcurrent: 10  # High concurrency
  useHeadlessBrowser: false  # Use JSDOM when possible
```

### **For Standard E-commerce Sites**
```yaml
behavior:
  rateLimit: 100     # Balanced
  maxConcurrent: 5   # Moderate concurrency
  useHeadlessBrowser: false  # Use JSDOM when possible
```

### **For Complex Sites (JavaScript-heavy, variations)**
```yaml
behavior:
  rateLimit: 200     # Conservative
  maxConcurrent: 3   # Lower concurrency
  useHeadlessBrowser: true   # Use Puppeteer when needed
```

## üìà **Performance Monitoring**

### **New Endpoint**
```
GET /api/scrape/performance
```

### **Metrics Available**
- Total jobs processed
- Total products scraped
- Average time per product
- Total processing time
- Current active jobs
- Queue length

### **Example Response**
```json
{
  "success": true,
  "data": {
    "totalJobs": 15,
    "totalProducts": 1250,
    "averageTimePerProduct": 245.6,
    "totalProcessingTime": 307000,
    "currentActiveJobs": 2,
    "queueLength": 0,
    "isProcessing": true
  }
}
```

## üöÄ **Additional Optimization Tips**

### **1. Use JSDOM When Possible**
- Set `useHeadlessBrowser: false` for simple sites
- JSDOM is 10-20x faster than Puppeteer

### **2. Minimize Selector Waiting**
- Only wait for essential selectors
- Use `waitForSelectors` sparingly

### **3. Batch Processing**
- Process products in batches rather than individually
- Use `maxConcurrent` to control batch size

### **4. Resource Blocking**
- Block images, CSS, and fonts when using Puppeteer
- Focus only on HTML content

### **5. Smart Fallbacks**
- Use fallback selectors to avoid waiting
- Implement graceful degradation

## üîç **Troubleshooting Performance Issues**

### **If Scraping is Still Slow:**
1. Check `rateLimit` settings in recipe
2. Verify `maxConcurrent` is > 1
3. Consider disabling `useHeadlessBrowser`
4. Monitor performance metrics via `/api/scrape/performance`
5. Check network latency to target site

### **If Getting Blocked:**
1. Increase `rateLimit` to 500-1000ms
2. Reduce `maxConcurrent` to 2-3
3. Enable `useHeadlessBrowser` for better stealth
4. Add random delays between requests

## üìù **Recipe Template for Maximum Performance**

```yaml
behavior:
  waitForSelectors:
    - "h1"           # Essential selectors only
    - ".price"
  scrollToLoad: false
  useHeadlessBrowser: false  # Use JSDOM for speed
  rateLimit: 50       # Very aggressive
  maxConcurrent: 10   # High concurrency
  waitForVariations: false   # Disable if not needed
  waitForDynamicContent: false
  waitForJavaScript: false
```

## üéØ **Next Steps for Further Optimization**

1. **Implement connection pooling** for HTTP requests
2. **Add caching layer** for repeated requests
3. **Use CDN proxies** for geographic distribution
4. **Implement adaptive rate limiting** based on response times
5. **Add request queuing** with priority levels

---

**Note**: These optimizations maintain scraping quality while significantly improving speed. Monitor your target sites to ensure they don't block the increased request frequency.
